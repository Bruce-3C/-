# æœ€çµ‚ç‰ˆtrain_experiments.py
# ã€Œä¸€éµå…¨æ¨¡å‹è¨“ç·´ç‰ˆæœ¬ã€çš„ train_experiments.pyï¼Œå¯ä»¥å–®ç¨è¨“ç·´æŸä¸€å€‹æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç”¨ --run_all è‡ªå‹•ä¾åºè¨“ç·´ AlexNetã€GoogLeNetã€ResNet-50ã€EfficientNet-B0ã€MobileNet-V3_Largeï¼Œæœ€å¾Œé‚„æœƒè‡ªå‹•åŒ¯å‡ºä¸€ä»½æ¯”è¼ƒè¡¨ï¼ˆCSVï¼‰ã€‚
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, datasets, transforms
from torch.utils.data import DataLoader
from train_model import train_model
import pandas as pd
import time
import os

# ------------------------
# 1. è³‡æ–™åŠ è¼‰å‡½å¼
# ------------------------
def get_dataloaders(data_dir, batch_size=32):
    transform = {
        "train": transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ]),
        "val": transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])
    }

    image_datasets = {
        "train": datasets.ImageFolder(os.path.join(data_dir, "train"), transform=transform["train"]),
        "val": datasets.ImageFolder(os.path.join(data_dir, "val"), transform=transform["val"])
    }

    dataloaders = {
        "train": DataLoader(image_datasets["train"], batch_size=batch_size, shuffle=True, num_workers=0),
        "val": DataLoader(image_datasets["val"], batch_size=batch_size, shuffle=False, num_workers=0)
    }

    num_classes = len(image_datasets["train"].classes)
    print(f"ğŸ“Š Detected {num_classes} classes: {image_datasets['train'].classes}")
    return dataloaders, num_classes


# ------------------------
# 2. æ¨¡å‹å»ºç«‹å‡½å¼
# ------------------------
def build_model(model_name, num_classes):
    if model_name == "alexnet":
        model = models.alexnet(weights="IMAGENET1K_V1")
        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)
    elif model_name == "googlenet":
        model = models.googlenet(weights="IMAGENET1K_V1")
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    elif model_name == "resnet50":
        model = models.resnet50(weights="IMAGENET1K_V1")
        model.fc = nn.Linear(model.fc.in_features, num_classes)
    elif model_name == "efficientnet_b0":
        model = models.efficientnet_b0(weights="IMAGENET1K_V1")
        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)
    elif model_name == "mobilenet_v3_large":
         model = models.mobilenet_v3_large(weights="IMAGENET1K_V1")
         model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)
    else:
        raise ValueError(f"âŒ Unsupported model: {model_name}")
    return model


# ------------------------
# 3. è¨“ç·´æµç¨‹ (ä¿®æ­£å¾Œçš„ç‰ˆæœ¬)
# ------------------------
def run_training(model_name, data_dir, batch_size, lr, epochs, device):
    dataloaders, num_classes = get_dataloaders(data_dir, batch_size)
    model = build_model(model_name, num_classes)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    start = time.time()
    
    # ğŸ’¥ ä¿®æ­£ï¼šå‡è¨­ train_model ç¾åœ¨å›å‚³ (æ¨¡å‹, çµæœå­—å…¸)
    # å¦‚æœä½ çš„ train_model åªå›å‚³æ¨¡å‹ï¼Œé€™å€‹ç¨‹å¼ç¢¼æœƒå´©æ½°ï¼
    # ä½ å¿…é ˆä¿®æ”¹ train_model.py
    trained_model, result = train_model(model_name, model, dataloaders, criterion, optimizer,
                                        num_epochs=epochs, device=device)
    
    elapsed = (time.time() - start) / 60
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå­—å…¸ï¼Œå¦‚æœ train_model å›å‚³éŒ¯èª¤ï¼Œé€™è£¡æœƒä¿®æ­£
    if not isinstance(result, dict):
        # é€™æ˜¯æ‡‰æ€¥ä»£ç¢¼ï¼šå¦‚æœ train_model åªå›å‚³äº†æ¨¡å‹ï¼Œæˆ‘å€‘éœ€è¦æ‰‹å‹•å»ºç«‹å­—å…¸ï¼Œä½†ç„¡æ³•å–å¾—æœ€ä½³ F1
        result = {} 
        # å»ºè­°ï¼šç¢ºä¿ train_model å›å‚³æœ€ä½³ F1
        
    result["train_time(min)"] = round(elapsed, 2)
    
    # æˆ‘å€‘ä¸å†éœ€è¦å›å‚³æ¨¡å‹ï¼Œå› ç‚ºå®ƒå·²ç¶“è¢«å„²å­˜åˆ°æª”æ¡ˆ
    return result

# ------------------------
# 4. ä¸»ç¨‹å¼å…¥å£
# ------------------------
def main(args):
    os.makedirs("results", exist_ok=True)
    summary = []

    # å–®æ¨¡å‹è¨“ç·´
    if not args.run_all:
        res = run_training(args.model, args.data_dir, args.batch_size,
                           args.lr, args.epochs, args.device)
        summary.append({"Model": args.model, **res})
    else:
        models_to_run = ["alexnet", "googlenet", "resnet50", "efficientnet_b0", "mobilenet_v3_large"]
        print(f"ğŸš€ é–‹å§‹ä¾åºè¨“ç·´ {len(models_to_run)} å€‹æ¨¡å‹...")
        for name in models_to_run:
            res = run_training(name, args.data_dir, args.batch_size,
                               args.lr, args.epochs, args.device)
            summary.append({"Model": name, **res})

    # åŒ¯å‡ºçµæœ
    df = pd.DataFrame(summary)
    csv_path = os.path.join("results", f"summary_{time.strftime('%Y%m%d_%H%M%S')}.csv")
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"\nğŸ“„ æ‰€æœ‰çµæœå·²åŒ¯å‡ºåˆ° {csv_path}")
    print(df)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, required=True, help="Dataset path (must contain train/ and val/)")
    parser.add_argument("--model", type=str, default="efficientnet_b0",
                        choices=["alexnet", "googlenet", "resnet50", "efficientnet_b0", "mobilenet_v3_large"])
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--run_all", action="store_true", help="Train all five models sequentially")
    args = parser.parse_args()

    main(args)
